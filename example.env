##################################
# Database variables
##################################
# Use this user to connect to MySQL
MYSQL_SEARCH_ENGINE_CONNECTION_USER=root
MYSQL_SEARCH_ENGINE_CONNECTION_PASSWORD=root
MYSQL_SEARCH_ENGINE_CONNECTION_HOST=mysql
MYSQL_SEARCH_ENGINE_CONNECTION_PORT=3306
MYSQL_SEARCH_ENGINE_DATABASE=tuesearch

# Log file path for database operations.
DATABASE_LOG_FILE=/opt/tuesearch/log/database.log

##################################
# Seeds initialization variables
##################################

# Output file path for storing search engine results.
SERP_FILE=/app/crawler/data/serp.json

# The page number to start fetching search results from.
SERP_GOOGLE_PAGE=1

# API key for accessing the search engine results page (SERP) API.
SERP_API_KEY=<Put your key here if you want a new serp.json. A free key can be obtained at https://serper.dev>

# Number of search results to fetch per page.
SERP_GOOGLE_PAGE_SIZE=50

# Topics or keywords to search for in the SERP.
SERP_TOPICS='["attractions","history","events","culture","university","food","transportation","accommodation","nature","activities","education","landmarks","blog","experience","people","famous people","academy","community","hobby","study","neckar","adventure"]'

# Log file path for fetching search engine results.
SERP_LOG_FILE=/opt/tuesearch/log/fetch_serp.log

##################################
# Queue initialization variables
##################################

# Log file path for initializing the crawling queue.
QUEUE_LOG_FILE=/opt/tuesearch/log/initialize_queue.log

# List of domains to exclude from the initial crawling queue.
QUEUE_INITIAL_BLACK_LIST='["indeed","flickr","youtube","tiktok","pinterest","linkedin","facebook","instagram","tripadvisor","yelp","thefork","kamoot","twitter","google"]'

# List of manually added seed URLs for the crawling queue.
QUEUE_MANUAL_SEEDS='["https://www.kloster-bebenhausen.de/en/","https://www.tuebingen.de/en/","https://www.tuebingen-info.de/en/","https://en.wikipedia.org/wiki/T%C3%BCbingen","https://uni-tuebingen.de/en/university/news-and-publications/press-releases/","https://uni-tuebingen.de/en/university/news-and-publications/","https://tunewsinternational.com/category/news-in-english/","https://www.komoot.com/guide/883/road-cycling-routes-around-tuebingen","https://www.thelocal.de/tag/tubingen","https://tuebingenresearchcampus.com/en/news/","https://www.my-stuwe.de/en/news/","https://www.germany.travel/en/cities-culture/tuebingen.html","https://tuebingenresearchcampus.com/en/tuebingen/living-in-tuebingen/going-to-the-doctor/","https://en.wikivoyage.org/wiki/T%C3%BCbingen"]'

##################################
# Crawling variables
##################################

# Log file path for crawling operations.
CRAWL_LOG_FILE=/opt/tuesearch/log/crawl.log

# Threshold value for classifying English content in a document.
CRAWL_ENGLISH_CLASSIFICATION_THRESHOLD=0.3

# Threshold value for classifying English content in a document when multiply languages are detected.
# In this case, prob must be 1 / n + this threshold.
CRAWL_ENGLISH_CLASSIFICATION_MULTI_THRESHOLD=0.1

# Number of jobs to be crawled in a single batch.
CRAWL_BATCH_SIZE=32

# Timeout (in seconds) for each HTTP request during crawling.
CRAWL_TIMEOUT=5

# Time to render objects after the page is loaded (in seconds).
CRAWL_RENDER_TIMEOUT=8

# Number of retries for failed HTTP requests during crawling.
CRAWL_RETRIES=3

# List of HTTP status codes to retry the request if encountered during crawling.
CRAWL_RETRIES_IF_STATUS='[500, 502, 503, 504]'

# Backoff factor for exponential backoff during retries. Exponential seems to be good,
# especially for bad connection.
CRAWL_BACKOFF_FACTOR=1

# How many redirect do we accept before we stop following them.
CRAWL_REDIRECTION_LIMIT=5

# Raise error on status. Set to false to deactivate noisy error of requests and use our own nice.
CRAWL_RAISE_ON_STATUS=false

# List of domains to exclude from the crawling process.
CRAWL_BLACK_LIST='["airbnb","yahoo","doi","swr","alamy","free-apply","nih","flickr","youtube","tiktok","pinterest","linkedin","facebook","twitter","google","springer","instagram","amazon","semanticscholar","gea","kreis-tuebingen","whatsapp","adscientificindex","tagblatt"]'

# List of file extensions to exclude from crawling.
CRAWL_EXCLUDED_EXTENSIONS='["js","css","jpg","jpeg","png","gif","pdf","ogg"]'

# Sleep randomly to avoid detection...
CRAWL_RANDOM_SLEEP_INTERVAL='[1,3]'

##################################
# Spacy environment variables
##################################
# Spacy model for initial tokenizing.
SPACY_TOKENIZER_MODEL=en_core_web_md

# Spacy model for stemming english.
SPACY_ENGLISH_LEMMATIZATION_MODEL=en_core_web_md

# Ignore words having length larger than this
REMOVE_LONG_WORD_THRESHOLD=15

##################################
# Invertex index variables
##################################

# File path of the invertex index
INVERTED_INDEX_FILE=/opt/tuesearch/data/inverted_index.pickle

# File path of the invertex index
INVERTED_INDEX_LOG_FILE=/opt/tuesearch/log/inverted_index.log

# Every possible Tübingen writing styles
INVERTED_INDEX_TUEBINGEN_WRITING_STYLES='["tuebingen", "tübingen", "tubingen", "tã¼bingen"]'

##################################
#  Ranking variables
##################################

# Where to store log file of rank builder
BUILD_RANKER_LOG_FILE=/opt/tuesearch/log/rank_builder.log

# Where to place the TFIDF vectorizer
TFIDF_VECTORIZER_FILE=/opt/tuesearch/data/tfidf.pickle

# NGram parameter of the TFIDF vectorizer
TFIDF_NGRAM_RANGE='[1,2]'

# Where the directed link graph will be stored.
DIRECTED_LINK_GRAPH_FILE=/opt/tuesearch/data/directed_link_graph.pickle

# PageRank file
PAGERANK_FILE=/opt/tuesearch/data/pagerank.json

# PageRank initialization
PAGERANK_PERSONALIZATION='{ "tuebingen": 0.1, "wikipedia": 0.1 }'

# PageRank max_iter
PAGERANK_MAX_ITER=1000
