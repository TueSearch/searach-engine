# Output directory where the models and logs go
# Should not be the same with working directory
# Since that would slow down the IDE.
OUTPUT_DIR=/opt/tuesearch

##################################
# Lock variables
##################################
LOCK_RETRY=5
LOCK_TIMEOUT=5
LOCK_FILE_PATH=${OUTPUT_DIR}/lock.lock
LOCK_RETRY_INTERVAL=1

##################################
# Database variables
##################################
# Experiment with different strategies
# Store each priority queue's strategy in a different database
MYSQL_DATABASE=get_highest_priority_jobs
MYSQL_USER=tuesearch
MYSQL_PASSWORD=tuesearch
MYSQL_ROOT_PASSWORD=root
MYSQL_PORT=3306
MYSQL_HOST=mysql
PMA_HOST=${MYSQL_HOST}
PMA_PORT=${MYSQL_PORT}
PMA_ARBITRARY=1
PMA_PASSWORD=pma_password

##################################
# Variables to configure the
# communication process
# between worker and master
##################################
# Where to retrieve jobs
CRAWLER_MANAGER_PORT=6000

# Where to retrieve jobs
CRAWLER_MANAGER_HOST=http://manager:${CRAWLER_MANAGER_PORT}

# Password
CRAWLER_MANAGER_PASSWORD=pw

# How many jobs to request from the manager at once.
# At most.
CRAWLER_MANAGER_MAX_JOB_REQUESTS=16

# Number of jobs to be crawled in a single batch.
# The higher the number,
# the less frequent the manager will ask for new jobs.
CRAWL_WORKER_BATCH_SIZE=8

# Timeout (in seconds) when communicate with crawler manager.
CRAWLER_WORKER_TIMEOUT=30

##################################
# Crawling variables
##################################
# Text around the link to be crawled.
CRAWL_SURROUNDING_TEXT_LENGTH=10

# Every possible Tübingen writing styles
TUEBINGEN_WRITING_STYLES='["tubingen", "tuebingen", "tübingen", "tã¼bingen"]'

# Threshold value for classifying English content in a document.
CRAWL_ENGLISH_CLASSIFICATION_THRESHOLD=0.3

# Threshold value for classifying English content in a document when multiply languages are detected.
# In this case, prob must be 1 / n + this threshold.
CRAWL_ENGLISH_CLASSIFICATION_MULTI_THRESHOLD=0.1

# Timeout (in seconds) for each HTTP request during crawling.
CRAWL_TIMEOUT=5

# Time to render objects after the page is loaded (in seconds).
CRAWL_RENDER_TIMEOUT=8

# Number of retries for failed HTTP requests during crawling.
CRAWL_RETRIES=3

# List of HTTP status codes to retry the request if encountered during crawling.
CRAWL_RETRIES_IF_STATUS='[500, 502, 503, 504]'

# Backoff factor for exponential backoff during retries. Exponential seems to be good,
# especially for bad connection.
CRAWL_BACKOFF_FACTOR=1

# How many redirect do we accept before we stop following them.
CRAWL_REDIRECTION_LIMIT=5

# Raise error on status. Set to false to deactivate noisy error of requests and use our own nice.
CRAWL_RAISE_ON_STATUS=false

# List of file extensions to exclude from crawling.
CRAWL_EXCLUDED_EXTENSIONS='["ics","js","css","jpg","jpeg","png","gif","pdf","ogg"]'

##################################
# Spacy environment variables
##################################
# Spacy model for initial tokenizing.
SPACY_MODEL=en_core_web_md

# Ignore words having length larger than this
REMOVE_LONG_WORD_THRESHOLD=15

##################################
# Invertex index variables
##################################

# File path of the invertex index
INDEX_FILE=${OUTPUT_DIR}/index.pickle

##################################
#  Ranking variables
##################################

# Where to place the TFIDF vectorizer
TFIDF_VECTORIZER_FILE=${OUTPUT_DIR}/tfidf.pickle

# NGram parameter of the TFIDF vectorizer
TFIDF_NGRAM_RANGE='[1,2]'

# Where the directed link graph will be stored.
DIRECTED_LINK_GRAPH_FILE=${OUTPUT_DIR}/directed_link_graph.pickle

# PageRank file
PAGERANK_FILE=${OUTPUT_DIR}/pagerank.json

# PageRank initialization
PAGERANK_PERSONALIZATION='{ "tuebingen": 0.1, "wikipedia": 0.1 }'

# PageRank max_iter
PAGERANK_MAX_ITER=1000